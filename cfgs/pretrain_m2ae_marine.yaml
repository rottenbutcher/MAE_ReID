optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.001,
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 200,
    initial_epochs : 10
}}

dataset : {
  train : { _base_: cfgs/dataset_configs/Simulation_Ship.yaml,
            others: {subset: 'train', npoints: 8192}},
  val : { _base_: cfgs/dataset_configs/Simulation_Ship.yaml,
            others: {subset: 'test', npoints: 8192}},
  }

model : {
  NAME: Point_M2AE, # ★ 모델 이름 변경
  loss: cdl2,
  mask_ratio: 0.7,     # ★ M2AE는 mask_ratio를 직접 사용
  drop_path_rate: 0.1,
  num_heads: 6,

  # --- Point-M2AE 계층 구조 설정 ---
  # 8192 포인트 -> 1024 그룹 -> 256 그룹 (2-level)
  group_sizes: [32, 32],      # 각 레벨의 KNN 그룹 크기
  num_groups: [1024, 256],  # 각 레벨의 그룹(센터) 수

  # --- Encoder (Point-MAE의 12-depth, 384-dim과 유사하게 맞춤)
  encoder_dims: [192, 384],   # 각 인코더 레벨의 차원
  encoder_depths: [4, 8],     # 각 인코더 레벨의 깊이 (총 12)
  local_radius: [-1, -1],   # Pretrain 시 Local Attention 비활성화 (-1)

  # --- Decoder (Pretrain용 경량 디코더)
  decoder_dims: [384, 192],   # 디코더 차원 (인코더와 맞춤)
  decoder_depths: [2, 2],     # 디코더 깊이
  decoder_up_blocks: [2],     # 256 -> 1024 업샘플링 시 PointNet 블록 수
}

npoints: 8192
total_bs : 64
step_per_update : 1
max_epoch : 200